{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiclassification using ANN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUUaH0pDy8Nx",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "6e848060-d68e-4735-f24a-f11e0b1232ac"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5245caf9-1578-4e50-8a7c-4aaefa4b193e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5245caf9-1578-4e50-8a7c-4aaefa4b193e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving iris.csv to iris.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6U2LagizqrG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2795db19-a0ac-4229-a423-168d503e8d77"
      },
      "source": [
        "uploaded\n",
        "import pandas as pd\n",
        "import io\n",
        "data=pd.read_csv(io.StringIO(uploaded['iris.csv'].decode('utf-8')))\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width        class\n",
              "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
              "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
              "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
              "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
              "4           5.0          3.6           1.4          0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0ALWrwXzqwT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "170690c9-6deb-4ff6-96cd-ef6dd39cf9af"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   sepal_length  150 non-null    float64\n",
            " 1   sepal_width   150 non-null    float64\n",
            " 2   petal_length  150 non-null    float64\n",
            " 3   petal_width   150 non-null    float64\n",
            " 4   class         150 non-null    object \n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 6.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf0u2J71zq0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzdbBn-Tzq4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X=data.iloc[:,:-1].values\n",
        "Y=data.iloc[:,-1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVcC3fcq383n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c3bb10b2-384e-4e1e-d914-3ce75c54149c"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.1, 2.8, 4.7, 1.2],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [7.1, 3. , 5.9, 2.1],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.9, 3. , 5.1, 1.8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFRykkhQzrAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_X_1=LabelEncoder()\n",
        "y1=labelencoder_X_1.fit_transform(Y)\n",
        "y=pd.get_dummies(y1).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoPMub6azqu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEbU0SdczqpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc=StandardScaler()\n",
        "X_train=sc.fit_transform(X_train)\n",
        "X_test=sc.fit_transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axwptMbY0J5j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dade1626-ff61-4dbe-8c54-7fe44437aa30"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.17513208, -1.94010658,  0.70755192,  0.40095423],\n",
              "       [ 1.61008527, -0.09791192,  1.15797344,  0.53185703],\n",
              "       [ 0.17513208, -0.09791192,  0.59494654,  0.79366261],\n",
              "       [ 0.41429095, -0.32818625,  0.31343309,  0.13914864],\n",
              "       [-0.42276508, -1.47955792,  0.03191964, -0.12265695],\n",
              "       [-0.06402678, -0.78873492,  0.76385461,  0.92456541],\n",
              "       [ 1.01218811,  0.13236241,  1.04536806,  1.57907938],\n",
              "       [ 0.41429095, -0.55846059,  0.59494654,  0.79366261],\n",
              "       [-0.18360621, -1.01900925, -0.13698843, -0.25355974],\n",
              "       [-1.49897997,  1.28373407, -1.54455569, -1.3007821 ],\n",
              "       [ 1.25134697,  0.13236241,  0.93276268,  1.186371  ],\n",
              "       [ 0.65344981, -0.32818625,  0.31343309,  0.13914864],\n",
              "       [-1.2598211 , -0.09791192, -1.31934493, -1.16987931],\n",
              "       [-0.42276508,  1.05345974, -1.37564762, -1.3007821 ],\n",
              "       [-0.54234451,  1.97455707, -1.15043686, -1.03897651],\n",
              "       [-0.90108281,  1.05345974, -1.31934493, -1.16987931],\n",
              "       [ 1.49050583, -0.09791192,  1.21427613,  1.186371  ],\n",
              "       [ 1.01218811,  0.59291108,  1.10167075,  1.186371  ],\n",
              "       [ 1.13176754, -0.55846059,  0.59494654,  0.27005144],\n",
              "       [ 0.65344981, -0.55846059,  1.04536806,  1.31727379],\n",
              "       [ 0.77302924, -0.09791192,  0.98906537,  0.79366261],\n",
              "       [-0.18360621, -0.09791192,  0.2571304 ,  0.00824585],\n",
              "       [-1.02066224,  0.59291108, -1.31934493, -1.3007821 ],\n",
              "       [ 1.01218811,  0.13236241,  0.36973578,  0.27005144],\n",
              "       [ 0.17513208, -1.94010658,  0.14452502, -0.25355974],\n",
              "       [ 0.65344981, -0.78873492,  0.87645999,  0.92456541],\n",
              "       [-1.37940054,  0.36263674, -1.20673955, -1.3007821 ],\n",
              "       [-1.49897997,  0.13236241, -1.26304224, -1.3007821 ],\n",
              "       [-0.90108281,  0.82318541, -1.26304224, -1.3007821 ],\n",
              "       [ 2.20798243,  1.74428274,  1.66469765,  1.31727379],\n",
              "       [-1.85771826, -0.09791192, -1.488253  , -1.4316849 ],\n",
              "       [ 0.53387038,  0.59291108,  1.27057882,  1.70998218],\n",
              "       [-0.42276508, -1.47955792, -0.02438305, -0.25355974],\n",
              "       [-1.02066224, -2.40065525, -0.13698843, -0.25355974],\n",
              "       [ 0.77302924, -0.55846059,  0.48234116,  0.40095423],\n",
              "       [-1.02066224,  0.82318541, -1.20673955, -1.03897651],\n",
              "       [-1.73813883, -0.32818625, -1.31934493, -1.3007821 ],\n",
              "       [-1.02066224,  1.05345974, -1.20673955, -0.77717092],\n",
              "       [ 1.01218811,  0.13236241,  0.53864385,  0.40095423],\n",
              "       [ 1.7296647 , -0.32818625,  1.43948689,  0.79366261],\n",
              "       [-0.42276508, -1.01900925,  0.36973578,  0.00824585],\n",
              "       [-0.78150338, -0.78873492,  0.08822233,  0.27005144],\n",
              "       [-0.18360621,  3.12592873, -1.26304224, -1.03897651],\n",
              "       [ 1.61008527,  0.36263674,  1.27057882,  0.79366261],\n",
              "       [-0.54234451,  0.82318541, -1.15043686, -1.3007821 ],\n",
              "       [-0.06402678,  2.2048314 , -1.43195031, -1.3007821 ],\n",
              "       [ 0.29471151, -1.01900925,  1.04536806,  0.27005144],\n",
              "       [-0.18360621, -1.24928358,  0.70755192,  1.0554682 ],\n",
              "       [-1.2598211 , -0.09791192, -1.31934493, -1.4316849 ],\n",
              "       [ 0.77302924, -0.09791192,  1.15797344,  1.31727379],\n",
              "       [-1.49897997,  0.36263674, -1.31934493, -1.3007821 ],\n",
              "       [ 0.89260867, -0.32818625,  0.48234116,  0.13914864],\n",
              "       [ 1.01218811, -0.09791192,  0.8201573 ,  1.44817659],\n",
              "       [ 0.29471151, -0.09791192,  0.48234116,  0.27005144],\n",
              "       [-0.18360621,  1.74428274, -1.15043686, -1.16987931],\n",
              "       [ 1.3709264 ,  0.36263674,  0.53864385,  0.27005144],\n",
              "       [-1.37940054,  0.36263674, -1.37564762, -1.3007821 ],\n",
              "       [ 0.77302924, -0.09791192,  0.8201573 ,  1.0554682 ],\n",
              "       [ 2.20798243, -1.01900925,  1.77730303,  1.44817659],\n",
              "       [-1.14024167, -1.47955792, -0.24959382, -0.25355974],\n",
              "       [-1.2598211 ,  0.13236241, -1.20673955, -1.3007821 ],\n",
              "       [-0.42276508,  2.66538007, -1.31934493, -1.3007821 ],\n",
              "       [-0.30318565, -0.09791192,  0.20082771,  0.13914864],\n",
              "       [ 0.65344981,  0.36263674,  0.87645999,  1.44817659],\n",
              "       [-0.78150338,  1.05345974, -1.26304224, -1.3007821 ],\n",
              "       [ 2.44714129,  1.74428274,  1.49578958,  1.0554682 ],\n",
              "       [-0.06402678, -0.55846059,  0.76385461,  1.57907938],\n",
              "       [-0.18360621, -0.32818625,  0.2571304 ,  0.13914864],\n",
              "       [-0.90108281,  1.51400841, -1.26304224, -1.03897651],\n",
              "       [-0.90108281, -1.24928358, -0.41850189, -0.12265695],\n",
              "       [-1.02066224,  0.82318541, -1.26304224, -1.3007821 ],\n",
              "       [ 2.20798243, -0.55846059,  1.66469765,  1.0554682 ],\n",
              "       [ 2.08840299, -0.09791192,  1.60839496,  1.186371  ],\n",
              "       [-1.02066224,  0.36263674, -1.43195031, -1.3007821 ],\n",
              "       [-0.54234451,  1.51400841, -1.26304224, -1.3007821 ],\n",
              "       [ 1.01218811,  0.59291108,  1.10167075,  1.70998218],\n",
              "       [ 0.29471151, -0.55846059,  0.14452502,  0.13914864],\n",
              "       [ 0.53387038, -1.24928358,  0.70755192,  0.92456541],\n",
              "       [ 1.84924413, -0.55846059,  1.32688151,  0.92456541],\n",
              "       [-1.02066224, -1.70983225, -0.24959382, -0.25355974],\n",
              "       [-0.54234451,  0.82318541, -1.26304224, -1.03897651],\n",
              "       [-0.54234451,  1.97455707, -1.37564762, -1.03897651],\n",
              "       [-0.30318565, -0.55846059,  0.65124923,  1.0554682 ],\n",
              "       [-1.14024167, -0.09791192, -1.31934493, -1.3007821 ],\n",
              "       [-0.66192394,  1.51400841, -1.26304224, -1.3007821 ],\n",
              "       [ 0.17513208, -0.78873492,  0.76385461,  0.53185703],\n",
              "       [ 1.13176754,  0.36263674,  1.21427613,  1.44817659],\n",
              "       [ 0.41429095,  0.82318541,  0.93276268,  1.44817659],\n",
              "       [-0.18360621, -0.55846059,  0.20082771,  0.13914864],\n",
              "       [ 0.53387038, -1.70983225,  0.36973578,  0.13914864],\n",
              "       [-1.14024167,  0.13236241, -1.26304224, -1.4316849 ],\n",
              "       [ 1.25134697,  0.13236241,  0.76385461,  1.44817659],\n",
              "       [-1.02066224,  1.28373407, -1.31934493, -1.3007821 ],\n",
              "       [-1.73813883, -0.09791192, -1.37564762, -1.3007821 ],\n",
              "       [ 0.17513208, -0.32818625,  0.42603847,  0.40095423],\n",
              "       [ 0.29471151, -0.55846059,  0.53864385,  0.00824585],\n",
              "       [ 0.65344981, -0.55846059,  1.04536806,  1.186371  ],\n",
              "       [-1.14024167,  0.13236241, -1.26304224, -1.3007821 ],\n",
              "       [-0.30318565, -0.32818625, -0.08068574,  0.13914864],\n",
              "       [ 0.05555265, -0.09791192,  0.2571304 ,  0.40095423],\n",
              "       [ 0.53387038, -0.78873492,  0.65124923,  0.79366261],\n",
              "       [ 1.13176754, -0.09791192,  0.98906537,  1.186371  ],\n",
              "       [-0.42276508, -1.70983225,  0.14452502,  0.13914864],\n",
              "       [ 0.53387038, -0.55846059,  0.76385461,  0.40095423],\n",
              "       [-1.2598211 ,  0.82318541, -1.03783148, -1.3007821 ],\n",
              "       [ 0.53387038,  0.59291108,  0.53864385,  0.53185703],\n",
              "       [-0.30318565, -1.24928358,  0.08822233, -0.12265695],\n",
              "       [-0.90108281,  1.74428274, -1.03783148, -1.03897651],\n",
              "       [-0.90108281,  1.74428274, -1.26304224, -1.16987931],\n",
              "       [-1.49897997,  0.82318541, -1.31934493, -1.16987931],\n",
              "       [-0.18360621, -0.55846059,  0.42603847,  0.13914864],\n",
              "       [ 0.89260867, -0.09791192,  0.36973578,  0.27005144],\n",
              "       [-0.90108281,  1.05345974, -1.31934493, -1.3007821 ],\n",
              "       [ 2.20798243, -0.09791192,  1.32688151,  1.44817659],\n",
              "       [ 0.29471151, -0.09791192,  0.65124923,  0.79366261],\n",
              "       [ 0.41429095, -1.94010658,  0.42603847,  0.40095423],\n",
              "       [ 0.53387038,  0.82318541,  1.04536806,  1.57907938],\n",
              "       [-1.73813883,  0.36263674, -1.37564762, -1.3007821 ],\n",
              "       [ 0.77302924,  0.36263674,  0.76385461,  1.0554682 ],\n",
              "       [-0.42276508, -1.24928358,  0.14452502,  0.13914864],\n",
              "       [ 0.53387038, -1.24928358,  0.65124923,  0.40095423],\n",
              "       [-0.90108281,  0.59291108, -1.15043686, -0.90807372],\n",
              "       [-0.06402678, -0.78873492,  0.76385461,  0.92456541],\n",
              "       [-0.06402678, -1.01900925,  0.14452502,  0.00824585],\n",
              "       [ 0.53387038, -0.32818625,  1.04536806,  0.79366261],\n",
              "       [-0.06402678, -0.78873492,  0.20082771, -0.25355974],\n",
              "       [-1.02066224, -0.09791192, -1.20673955, -1.3007821 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1QevizY4V6I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4503d821-c369-4801-83e9-8b781586ae88"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnZM4njQ4VLx",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-b6-8xr0J8K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "610390a6-7967-453a-ee16-58bbc499c825"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Activation,Dropout"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTe2raVs0KBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW_pGbdm0KHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def built_classifier():\n",
        "\n",
        "  model=Sequential()\n",
        "  model.add(Dense(16,kernel_initializer='uniform',activation='relu',input_dim=4))\n",
        "  # model.add(Dropout(0.2))\n",
        "  # model.add(Dense(4,kernel_initializer='uniform',activation='relu'))\n",
        "  model.add(Dense(3,kernel_initializer='uniform',activation='softmax'))\n",
        "  model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model= KerasClassifier(build_fn=built_classifier,batch_size=10,epochs=100)\n",
        "accuracies=cross_val_score(estimator= model,X=X_train,y=y_train,cv=4,n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AEwDYAY0KL6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f94cbc12-a6e6-4b9d-8b67-b1c81b6330de"
      },
      "source": [
        "accuracies"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.96875   , 0.90625   , 0.96875   , 0.96774191])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c4S78Zm0KFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1=Sequential()\n",
        "model1.add(Dense(16,kernel_initializer='uniform',activation='relu',input_dim=4))\n",
        "model1.add(Dense(3,kernel_initializer='uniform',activation='softmax'))\n",
        "\n",
        "model1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6Lrg_jh0KAU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "307355b6-84c5-4fda-f7cb-57182031216c"
      },
      "source": [
        "model1.fit(X_train,y_train,batch_size=10,epochs=100,validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 101 samples, validate on 26 samples\n",
            "Epoch 1/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 1.0980 - accuracy: 0.3000Train on 101 samples, validate on 26 samples\n",
            "Epoch 1/100\n",
            "101/101 [==============================] - 0s 1ms/step - loss: 1.0956 - accuracy: 0.6040 - val_loss: 1.0920 - val_accuracy: 0.8077\n",
            "101/101 [==============================] - 0s 1ms/step - loss: 1.0956 - accuracy: 0.6040 - val_loss: 1.0920 - val_accuracy: 0.8077\n",
            "Epoch 2/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 1.0896 - accuracy: 0.9000Epoch 2/100\n",
            "101/101 [==============================] - 0s 256us/step - loss: 1.0872 - accuracy: 0.7624 - val_loss: 1.0831 - val_accuracy: 0.8077\n",
            "101/101 [==============================] - 0s 256us/step - loss: 1.0872 - accuracy: 0.7624 - val_loss: 1.0831 - val_accuracy: 0.8077\n",
            "Epoch 3/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 1.0814 - accuracy: 0.8000Epoch 3/100\n",
            "101/101 [==============================] - 0s 184us/step - loss: 1.0738 - accuracy: 0.7426 - val_loss: 1.0688 - val_accuracy: 0.7308\n",
            "101/101 [==============================] - 0s 184us/step - loss: 1.0738 - accuracy: 0.7426 - val_loss: 1.0688 - val_accuracy: 0.7308\n",
            "Epoch 4/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 1.0635 - accuracy: 0.7000Epoch 4/100\n",
            "101/101 [==============================] - 0s 228us/step - loss: 1.0542 - accuracy: 0.7129 - val_loss: 1.0472 - val_accuracy: 0.6538\n",
            "101/101 [==============================] - 0s 228us/step - loss: 1.0542 - accuracy: 0.7129 - val_loss: 1.0472 - val_accuracy: 0.6538\n",
            "Epoch 5/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 1.0465 - accuracy: 0.6000Epoch 5/100\n",
            "101/101 [==============================] - 0s 231us/step - loss: 1.0264 - accuracy: 0.7228 - val_loss: 1.0184 - val_accuracy: 0.6923\n",
            "101/101 [==============================] - 0s 231us/step - loss: 1.0264 - accuracy: 0.7228 - val_loss: 1.0184 - val_accuracy: 0.6923\n",
            "Epoch 6/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.9625 - accuracy: 0.9000Epoch 6/100\n",
            "101/101 [==============================] - 0s 224us/step - loss: 0.9897 - accuracy: 0.7327 - val_loss: 0.9828 - val_accuracy: 0.6538\n",
            "101/101 [==============================] - 0s 224us/step - loss: 0.9897 - accuracy: 0.7327 - val_loss: 0.9828 - val_accuracy: 0.6538\n",
            "Epoch 7/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.9037 - accuracy: 0.9000Epoch 7/100\n",
            "101/101 [==============================] - 0s 258us/step - loss: 0.9446 - accuracy: 0.7327 - val_loss: 0.9384 - val_accuracy: 0.6154\n",
            "101/101 [==============================] - 0s 258us/step - loss: 0.9446 - accuracy: 0.7327 - val_loss: 0.9384 - val_accuracy: 0.6154\n",
            "Epoch 8/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.9348 - accuracy: 0.7000Epoch 8/100\n",
            "101/101 [==============================] - 0s 315us/step - loss: 0.8926 - accuracy: 0.7327 - val_loss: 0.8915 - val_accuracy: 0.6538\n",
            "101/101 [==============================] - 0s 315us/step - loss: 0.8926 - accuracy: 0.7327 - val_loss: 0.8915 - val_accuracy: 0.6538\n",
            "Epoch 9/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.7440 - accuracy: 1.0000Epoch 9/100\n",
            "101/101 [==============================] - 0s 242us/step - loss: 0.8343 - accuracy: 0.7327 - val_loss: 0.8433 - val_accuracy: 0.6154\n",
            "101/101 [==============================] - 0s 242us/step - loss: 0.8343 - accuracy: 0.7327 - val_loss: 0.8433 - val_accuracy: 0.6154\n",
            "Epoch 10/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.9031 - accuracy: 0.7000Epoch 10/100\n",
            "101/101 [==============================] - 0s 302us/step - loss: 0.7787 - accuracy: 0.7327 - val_loss: 0.7976 - val_accuracy: 0.6154\n",
            "101/101 [==============================] - 0s 302us/step - loss: 0.7787 - accuracy: 0.7327 - val_loss: 0.7976 - val_accuracy: 0.6154\n",
            "Epoch 11/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.8314 - accuracy: 0.6000Epoch 11/100\n",
            "101/101 [==============================] - 0s 229us/step - loss: 0.7260 - accuracy: 0.7426 - val_loss: 0.7565 - val_accuracy: 0.6154\n",
            "101/101 [==============================] - 0s 229us/step - loss: 0.7260 - accuracy: 0.7426 - val_loss: 0.7565 - val_accuracy: 0.6154\n",
            "Epoch 12/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.7088 - accuracy: 0.7000Epoch 12/100\n",
            "101/101 [==============================] - 0s 188us/step - loss: 0.6787 - accuracy: 0.7426 - val_loss: 0.7170 - val_accuracy: 0.6154\n",
            "101/101 [==============================] - 0s 188us/step - loss: 0.6787 - accuracy: 0.7426 - val_loss: 0.7170 - val_accuracy: 0.6154\n",
            "Epoch 13/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.6269 - accuracy: 0.8000Epoch 13/100\n",
            "101/101 [==============================] - 0s 221us/step - loss: 0.6348 - accuracy: 0.7426 - val_loss: 0.6829 - val_accuracy: 0.6154\n",
            "101/101 [==============================] - 0s 221us/step - loss: 0.6348 - accuracy: 0.7426 - val_loss: 0.6829 - val_accuracy: 0.6154\n",
            "Epoch 14/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.6024 - accuracy: 0.8000Epoch 14/100\n",
            "101/101 [==============================] - 0s 229us/step - loss: 0.5987 - accuracy: 0.7525 - val_loss: 0.6536 - val_accuracy: 0.6154\n",
            "101/101 [==============================] - 0s 229us/step - loss: 0.5987 - accuracy: 0.7525 - val_loss: 0.6536 - val_accuracy: 0.6154\n",
            "Epoch 15/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.5660 - accuracy: 0.8000Epoch 15/100\n",
            "101/101 [==============================] - 0s 212us/step - loss: 0.5685 - accuracy: 0.7525 - val_loss: 0.6285 - val_accuracy: 0.6154\n",
            "101/101 [==============================] - 0s 212us/step - loss: 0.5685 - accuracy: 0.7525 - val_loss: 0.6285 - val_accuracy: 0.6154\n",
            "Epoch 16/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.4545 - accuracy: 0.9000Epoch 16/100\n",
            "101/101 [==============================] - 0s 224us/step - loss: 0.5427 - accuracy: 0.7525 - val_loss: 0.6072 - val_accuracy: 0.6538\n",
            "101/101 [==============================] - 0s 224us/step - loss: 0.5427 - accuracy: 0.7525 - val_loss: 0.6072 - val_accuracy: 0.6538\n",
            "Epoch 17/100\n",
            "Epoch 17/100\n",
            "101/101 [==============================] - 0s 316us/step - loss: 0.5204 - accuracy: 0.7624 - val_loss: 0.5889 - val_accuracy: 0.6538\n",
            "101/101 [==============================] - 0s 316us/step - loss: 0.5204 - accuracy: 0.7624 - val_loss: 0.5889 - val_accuracy: 0.6538\n",
            "Epoch 18/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.4451 - accuracy: 0.8000Epoch 18/100\n",
            "101/101 [==============================] - 0s 306us/step - loss: 0.5023 - accuracy: 0.7624 - val_loss: 0.5753 - val_accuracy: 0.6538\n",
            "101/101 [==============================] - 0s 306us/step - loss: 0.5023 - accuracy: 0.7624 - val_loss: 0.5753 - val_accuracy: 0.6538\n",
            "Epoch 19/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.5136 - accuracy: 0.8000Epoch 19/100\n",
            "101/101 [==============================] - 0s 228us/step - loss: 0.4851 - accuracy: 0.7624 - val_loss: 0.5588 - val_accuracy: 0.6538\n",
            "101/101 [==============================] - 0s 228us/step - loss: 0.4851 - accuracy: 0.7624 - val_loss: 0.5588 - val_accuracy: 0.6538\n",
            "Epoch 20/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.5125 - accuracy: 0.7000Epoch 20/100\n",
            "101/101 [==============================] - 0s 245us/step - loss: 0.4714 - accuracy: 0.7723 - val_loss: 0.5451 - val_accuracy: 0.6923\n",
            "101/101 [==============================] - 0s 245us/step - loss: 0.4714 - accuracy: 0.7723 - val_loss: 0.5451 - val_accuracy: 0.6923\n",
            "Epoch 21/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.4867 - accuracy: 0.7000Epoch 21/100\n",
            "101/101 [==============================] - 0s 236us/step - loss: 0.4581 - accuracy: 0.7822 - val_loss: 0.5325 - val_accuracy: 0.7308\n",
            "101/101 [==============================] - 0s 236us/step - loss: 0.4581 - accuracy: 0.7822 - val_loss: 0.5325 - val_accuracy: 0.7308\n",
            "Epoch 22/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.6416 - accuracy: 0.8000Epoch 22/100\n",
            "101/101 [==============================] - 0s 208us/step - loss: 0.4472 - accuracy: 0.7921 - val_loss: 0.5204 - val_accuracy: 0.7692\n",
            "101/101 [==============================] - 0s 208us/step - loss: 0.4472 - accuracy: 0.7921 - val_loss: 0.5204 - val_accuracy: 0.7692\n",
            "Epoch 23/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.6005 - accuracy: 0.7000Epoch 23/100\n",
            "101/101 [==============================] - 0s 234us/step - loss: 0.4365 - accuracy: 0.8020 - val_loss: 0.5080 - val_accuracy: 0.7692\n",
            "101/101 [==============================] - 0s 234us/step - loss: 0.4365 - accuracy: 0.8020 - val_loss: 0.5080 - val_accuracy: 0.7692\n",
            "Epoch 24/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2869 - accuracy: 0.9000Epoch 24/100\n",
            "101/101 [==============================] - 0s 258us/step - loss: 0.4261 - accuracy: 0.8119 - val_loss: 0.4947 - val_accuracy: 0.8077\n",
            "101/101 [==============================] - 0s 258us/step - loss: 0.4261 - accuracy: 0.8119 - val_loss: 0.4947 - val_accuracy: 0.8077\n",
            "Epoch 25/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.3819 - accuracy: 0.7000Epoch 25/100\n",
            "101/101 [==============================] - 0s 280us/step - loss: 0.4167 - accuracy: 0.8119 - val_loss: 0.4829 - val_accuracy: 0.8077\n",
            "101/101 [==============================] - 0s 280us/step - loss: 0.4167 - accuracy: 0.8119 - val_loss: 0.4829 - val_accuracy: 0.8077\n",
            "Epoch 26/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.3659 - accuracy: 0.8000Epoch 26/100\n",
            "101/101 [==============================] - 0s 216us/step - loss: 0.4078 - accuracy: 0.8119 - val_loss: 0.4721 - val_accuracy: 0.8077\n",
            "101/101 [==============================] - 0s 216us/step - loss: 0.4078 - accuracy: 0.8119 - val_loss: 0.4721 - val_accuracy: 0.8077\n",
            "Epoch 27/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.5288 - accuracy: 0.7000Epoch 27/100\n",
            "101/101 [==============================] - 0s 207us/step - loss: 0.3995 - accuracy: 0.8119 - val_loss: 0.4613 - val_accuracy: 0.8077\n",
            "101/101 [==============================] - 0s 207us/step - loss: 0.3995 - accuracy: 0.8119 - val_loss: 0.4613 - val_accuracy: 0.8077\n",
            "Epoch 28/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.6730 - accuracy: 0.6000Epoch 28/100\n",
            "101/101 [==============================] - 0s 204us/step - loss: 0.3907 - accuracy: 0.8317 - val_loss: 0.4492 - val_accuracy: 0.8077\n",
            "101/101 [==============================] - 0s 204us/step - loss: 0.3907 - accuracy: 0.8317 - val_loss: 0.4492 - val_accuracy: 0.8077\n",
            "Epoch 29/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.3059 - accuracy: 0.9000Epoch 29/100\n",
            "101/101 [==============================] - 0s 245us/step - loss: 0.3824 - accuracy: 0.8416 - val_loss: 0.4392 - val_accuracy: 0.8077\n",
            "101/101 [==============================] - 0s 245us/step - loss: 0.3824 - accuracy: 0.8416 - val_loss: 0.4392 - val_accuracy: 0.8077\n",
            "Epoch 30/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1936 - accuracy: 0.9000Epoch 30/100\n",
            "101/101 [==============================] - 0s 254us/step - loss: 0.3749 - accuracy: 0.8515 - val_loss: 0.4294 - val_accuracy: 0.8462\n",
            "101/101 [==============================] - 0s 254us/step - loss: 0.3749 - accuracy: 0.8515 - val_loss: 0.4294 - val_accuracy: 0.8462\n",
            "Epoch 31/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.3833 - accuracy: 0.9000Epoch 31/100\n",
            "101/101 [==============================] - 0s 230us/step - loss: 0.3676 - accuracy: 0.8515 - val_loss: 0.4188 - val_accuracy: 0.8462\n",
            "101/101 [==============================] - 0s 230us/step - loss: 0.3676 - accuracy: 0.8515 - val_loss: 0.4188 - val_accuracy: 0.8462\n",
            "Epoch 32/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2852 - accuracy: 0.9000Epoch 32/100\n",
            "101/101 [==============================] - 0s 305us/step - loss: 0.3614 - accuracy: 0.8614 - val_loss: 0.4097 - val_accuracy: 0.8846\n",
            "101/101 [==============================] - 0s 305us/step - loss: 0.3614 - accuracy: 0.8614 - val_loss: 0.4097 - val_accuracy: 0.8846\n",
            "Epoch 33/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.4109 - accuracy: 0.9000Epoch 33/100\n",
            "101/101 [==============================] - 0s 304us/step - loss: 0.3550 - accuracy: 0.8713 - val_loss: 0.4040 - val_accuracy: 0.8462\n",
            "101/101 [==============================] - 0s 304us/step - loss: 0.3550 - accuracy: 0.8713 - val_loss: 0.4040 - val_accuracy: 0.8462\n",
            "Epoch 34/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2782 - accuracy: 0.9000Epoch 34/100\n",
            "101/101 [==============================] - 0s 265us/step - loss: 0.3472 - accuracy: 0.8614 - val_loss: 0.3972 - val_accuracy: 0.8846\n",
            "101/101 [==============================] - 0s 265us/step - loss: 0.3472 - accuracy: 0.8614 - val_loss: 0.3972 - val_accuracy: 0.8846\n",
            "Epoch 35/100\n",
            "Epoch 35/100\n",
            "101/101 [==============================] - 0s 252us/step - loss: 0.3417 - accuracy: 0.8614 - val_loss: 0.3884 - val_accuracy: 0.8846\n",
            "101/101 [==============================] - 0s 252us/step - loss: 0.3417 - accuracy: 0.8614 - val_loss: 0.3884 - val_accuracy: 0.8846\n",
            "Epoch 36/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1298 - accuracy: 1.0000Epoch 36/100\n",
            "101/101 [==============================] - 0s 290us/step - loss: 0.3344 - accuracy: 0.8614 - val_loss: 0.3793 - val_accuracy: 0.8846\n",
            "101/101 [==============================] - 0s 290us/step - loss: 0.3344 - accuracy: 0.8614 - val_loss: 0.3793 - val_accuracy: 0.8846\n",
            "Epoch 37/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2506 - accuracy: 0.8000Epoch 37/100\n",
            "101/101 [==============================] - 0s 248us/step - loss: 0.3278 - accuracy: 0.8614 - val_loss: 0.3696 - val_accuracy: 0.8846\n",
            "101/101 [==============================] - 0s 248us/step - loss: 0.3278 - accuracy: 0.8614 - val_loss: 0.3696 - val_accuracy: 0.8846\n",
            "Epoch 38/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.4267 - accuracy: 0.7000Epoch 38/100\n",
            "101/101 [==============================] - 0s 247us/step - loss: 0.3208 - accuracy: 0.8713 - val_loss: 0.3626 - val_accuracy: 0.8846\n",
            "101/101 [==============================] - 0s 247us/step - loss: 0.3208 - accuracy: 0.8713 - val_loss: 0.3626 - val_accuracy: 0.8846\n",
            "Epoch 39/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2517 - accuracy: 0.8000Epoch 39/100\n",
            "101/101 [==============================] - 0s 240us/step - loss: 0.3152 - accuracy: 0.8713 - val_loss: 0.3532 - val_accuracy: 0.8846\n",
            "101/101 [==============================] - 0s 240us/step - loss: 0.3152 - accuracy: 0.8713 - val_loss: 0.3532 - val_accuracy: 0.8846\n",
            "Epoch 40/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2998 - accuracy: 0.9000Epoch 40/100\n",
            "101/101 [==============================] - 0s 224us/step - loss: 0.3090 - accuracy: 0.8713 - val_loss: 0.3457 - val_accuracy: 0.8846\n",
            "101/101 [==============================] - 0s 224us/step - loss: 0.3090 - accuracy: 0.8713 - val_loss: 0.3457 - val_accuracy: 0.8846\n",
            "Epoch 41/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.4235 - accuracy: 0.8000Epoch 41/100\n",
            "101/101 [==============================] - 0s 210us/step - loss: 0.3037 - accuracy: 0.8713 - val_loss: 0.3398 - val_accuracy: 0.8846\n",
            "101/101 [==============================] - 0s 210us/step - loss: 0.3037 - accuracy: 0.8713 - val_loss: 0.3398 - val_accuracy: 0.8846\n",
            "Epoch 42/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2426 - accuracy: 1.0000Epoch 42/100\n",
            "101/101 [==============================] - 0s 237us/step - loss: 0.2988 - accuracy: 0.8713 - val_loss: 0.3343 - val_accuracy: 0.8846\n",
            "101/101 [==============================] - 0s 237us/step - loss: 0.2988 - accuracy: 0.8713 - val_loss: 0.3343 - val_accuracy: 0.8846\n",
            "Epoch 43/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2183 - accuracy: 0.9000Epoch 43/100\n",
            "101/101 [==============================] - 0s 283us/step - loss: 0.2939 - accuracy: 0.8713 - val_loss: 0.3291 - val_accuracy: 0.8846\n",
            "101/101 [==============================] - 0s 283us/step - loss: 0.2939 - accuracy: 0.8713 - val_loss: 0.3291 - val_accuracy: 0.8846\n",
            "Epoch 44/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1499 - accuracy: 1.0000Epoch 44/100\n",
            "101/101 [==============================] - 0s 210us/step - loss: 0.2898 - accuracy: 0.8812 - val_loss: 0.3238 - val_accuracy: 0.8846\n",
            "101/101 [==============================] - 0s 210us/step - loss: 0.2898 - accuracy: 0.8812 - val_loss: 0.3238 - val_accuracy: 0.8846\n",
            "Epoch 45/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.4210 - accuracy: 0.8000Epoch 45/100\n",
            "101/101 [==============================] - 0s 222us/step - loss: 0.2848 - accuracy: 0.8812 - val_loss: 0.3174 - val_accuracy: 0.8846\n",
            "101/101 [==============================] - 0s 222us/step - loss: 0.2848 - accuracy: 0.8812 - val_loss: 0.3174 - val_accuracy: 0.8846\n",
            "Epoch 46/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2037 - accuracy: 1.0000Epoch 46/100\n",
            "101/101 [==============================] - 0s 326us/step - loss: 0.2801 - accuracy: 0.8812 - val_loss: 0.3123 - val_accuracy: 0.8846\n",
            "101/101 [==============================] - 0s 326us/step - loss: 0.2801 - accuracy: 0.8812 - val_loss: 0.3123 - val_accuracy: 0.8846\n",
            "Epoch 47/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2300 - accuracy: 0.9000Epoch 47/100\n",
            "101/101 [==============================] - 0s 243us/step - loss: 0.2754 - accuracy: 0.8812 - val_loss: 0.3060 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 243us/step - loss: 0.2754 - accuracy: 0.8812 - val_loss: 0.3060 - val_accuracy: 0.9231\n",
            "Epoch 48/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1500 - accuracy: 1.0000Epoch 48/100\n",
            "101/101 [==============================] - 0s 225us/step - loss: 0.2705 - accuracy: 0.9010 - val_loss: 0.2994 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 225us/step - loss: 0.2705 - accuracy: 0.9010 - val_loss: 0.2994 - val_accuracy: 0.9231\n",
            "Epoch 49/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2741 - accuracy: 0.8000Epoch 49/100\n",
            "101/101 [==============================] - 0s 241us/step - loss: 0.2669 - accuracy: 0.9010 - val_loss: 0.2941 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 241us/step - loss: 0.2669 - accuracy: 0.9010 - val_loss: 0.2941 - val_accuracy: 0.9231\n",
            "Epoch 50/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.3153 - accuracy: 0.9000Epoch 50/100\n",
            "101/101 [==============================] - 0s 250us/step - loss: 0.2626 - accuracy: 0.9010 - val_loss: 0.2909 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 250us/step - loss: 0.2626 - accuracy: 0.9010 - val_loss: 0.2909 - val_accuracy: 0.9231\n",
            "Epoch 51/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2940 - accuracy: 0.9000Epoch 51/100\n",
            "101/101 [==============================] - 0s 246us/step - loss: 0.2587 - accuracy: 0.9010 - val_loss: 0.2870 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 246us/step - loss: 0.2587 - accuracy: 0.9010 - val_loss: 0.2870 - val_accuracy: 0.9231\n",
            "Epoch 52/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.3656 - accuracy: 0.8000Epoch 52/100\n",
            "101/101 [==============================] - 0s 243us/step - loss: 0.2550 - accuracy: 0.9010 - val_loss: 0.2832 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 243us/step - loss: 0.2550 - accuracy: 0.9010 - val_loss: 0.2832 - val_accuracy: 0.9231\n",
            "Epoch 53/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.4507 - accuracy: 0.9000Epoch 53/100\n",
            "101/101 [==============================] - 0s 245us/step - loss: 0.2515 - accuracy: 0.9010 - val_loss: 0.2794 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 245us/step - loss: 0.2515 - accuracy: 0.9010 - val_loss: 0.2794 - val_accuracy: 0.9231\n",
            "Epoch 54/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2266 - accuracy: 1.0000Epoch 54/100\n",
            "101/101 [==============================] - 0s 221us/step - loss: 0.2478 - accuracy: 0.8911 - val_loss: 0.2749 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 221us/step - loss: 0.2478 - accuracy: 0.8911 - val_loss: 0.2749 - val_accuracy: 0.9231\n",
            "Epoch 55/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1407 - accuracy: 1.0000Epoch 55/100\n",
            "101/101 [==============================] - 0s 228us/step - loss: 0.2438 - accuracy: 0.9010 - val_loss: 0.2698 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 228us/step - loss: 0.2438 - accuracy: 0.9010 - val_loss: 0.2698 - val_accuracy: 0.9231\n",
            "Epoch 56/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.3470 - accuracy: 0.8000Epoch 56/100\n",
            "101/101 [==============================] - 0s 202us/step - loss: 0.2403 - accuracy: 0.9010 - val_loss: 0.2656 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 202us/step - loss: 0.2403 - accuracy: 0.9010 - val_loss: 0.2656 - val_accuracy: 0.9231\n",
            "Epoch 57/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2044 - accuracy: 1.0000Epoch 57/100\n",
            "101/101 [==============================] - 0s 235us/step - loss: 0.2372 - accuracy: 0.9010 - val_loss: 0.2628 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 235us/step - loss: 0.2372 - accuracy: 0.9010 - val_loss: 0.2628 - val_accuracy: 0.9231\n",
            "Epoch 58/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.3015 - accuracy: 0.8000Epoch 58/100\n",
            "101/101 [==============================] - 0s 243us/step - loss: 0.2329 - accuracy: 0.9010 - val_loss: 0.2581 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 243us/step - loss: 0.2329 - accuracy: 0.9010 - val_loss: 0.2581 - val_accuracy: 0.9231\n",
            "Epoch 59/100\n",
            "Epoch 59/100\n",
            "101/101 [==============================] - 0s 244us/step - loss: 0.2292 - accuracy: 0.9109 - val_loss: 0.2566 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 244us/step - loss: 0.2292 - accuracy: 0.9109 - val_loss: 0.2566 - val_accuracy: 0.9231\n",
            "Epoch 60/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2421 - accuracy: 0.9000Epoch 60/100\n",
            "101/101 [==============================] - 0s 227us/step - loss: 0.2263 - accuracy: 0.9010 - val_loss: 0.2548 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 227us/step - loss: 0.2263 - accuracy: 0.9010 - val_loss: 0.2548 - val_accuracy: 0.9231\n",
            "Epoch 61/100\n",
            "Epoch 61/100\n",
            "101/101 [==============================] - 0s 211us/step - loss: 0.2238 - accuracy: 0.9010 - val_loss: 0.2513 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 211us/step - loss: 0.2238 - accuracy: 0.9010 - val_loss: 0.2513 - val_accuracy: 0.9231\n",
            "Epoch 62/100\n",
            " 10/101 [=>............................]Epoch 62/100\n",
            "101/101 [==============================] - 0s 247us/step - loss: 0.2202 - accuracy: 0.9109 - val_loss: 0.2494 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 247us/step - loss: 0.2202 - accuracy: 0.9109 - val_loss: 0.2494 - val_accuracy: 0.9231\n",
            "Epoch 63/100\n",
            "Epoch 63/100\n",
            "101/101 [==============================] - 0s 211us/step - loss: 0.2177 - accuracy: 0.9208 - val_loss: 0.2465 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 211us/step - loss: 0.2177 - accuracy: 0.9208 - val_loss: 0.2465 - val_accuracy: 0.9231\n",
            "Epoch 64/100\n",
            "Epoch 64/100\n",
            "101/101 [==============================] - 0s 347us/step - loss: 0.2146 - accuracy: 0.9208 - val_loss: 0.2435 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 347us/step - loss: 0.2146 - accuracy: 0.9208 - val_loss: 0.2435 - val_accuracy: 0.9231\n",
            "Epoch 65/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2029 - accuracy: 1.0000Epoch 65/100\n",
            "101/101 [==============================] - 0s 213us/step - loss: 0.2119 - accuracy: 0.9208 - val_loss: 0.2395 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 213us/step - loss: 0.2119 - accuracy: 0.9208 - val_loss: 0.2395 - val_accuracy: 0.9231\n",
            "Epoch 66/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2383 - accuracy: 0.9000Epoch 66/100\n",
            "101/101 [==============================] - 0s 237us/step - loss: 0.2097 - accuracy: 0.9208 - val_loss: 0.2350 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 237us/step - loss: 0.2097 - accuracy: 0.9208 - val_loss: 0.2350 - val_accuracy: 0.9231\n",
            "Epoch 67/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2153 - accuracy: 0.8000Epoch 67/100\n",
            "101/101 [==============================] - 0s 249us/step - loss: 0.2050 - accuracy: 0.9208 - val_loss: 0.2344 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 249us/step - loss: 0.2050 - accuracy: 0.9208 - val_loss: 0.2344 - val_accuracy: 0.9231\n",
            "Epoch 68/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2978 - accuracy: 0.7000Epoch 68/100\n",
            "101/101 [==============================] - 0s 258us/step - loss: 0.2024 - accuracy: 0.9208 - val_loss: 0.2331 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 258us/step - loss: 0.2024 - accuracy: 0.9208 - val_loss: 0.2331 - val_accuracy: 0.9231\n",
            "Epoch 69/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2378 - accuracy: 1.0000Epoch 69/100\n",
            "101/101 [==============================] - 0s 235us/step - loss: 0.1996 - accuracy: 0.9208 - val_loss: 0.2307 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 235us/step - loss: 0.1996 - accuracy: 0.9208 - val_loss: 0.2307 - val_accuracy: 0.9231\n",
            "Epoch 70/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2772 - accuracy: 0.8000Epoch 70/100\n",
            "101/101 [==============================] - 0s 220us/step - loss: 0.1971 - accuracy: 0.9208 - val_loss: 0.2282 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 220us/step - loss: 0.1971 - accuracy: 0.9208 - val_loss: 0.2282 - val_accuracy: 0.9231\n",
            "Epoch 71/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1530 - accuracy: 1.0000Epoch 71/100\n",
            "101/101 [==============================] - 0s 249us/step - loss: 0.1947 - accuracy: 0.9208 - val_loss: 0.2254 - val_accuracy: 0.9231\n",
            "101/101 [==============================] - 0s 249us/step - loss: 0.1947 - accuracy: 0.9208 - val_loss: 0.2254 - val_accuracy: 0.9231\n",
            "Epoch 72/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2992 - accuracy: 0.8000Epoch 72/100\n",
            "101/101 [==============================] - 0s 312us/step - loss: 0.1920 - accuracy: 0.9208 - val_loss: 0.2229 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 312us/step - loss: 0.1920 - accuracy: 0.9208 - val_loss: 0.2229 - val_accuracy: 0.9615\n",
            "Epoch 73/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1259 - accuracy: 1.0000Epoch 73/100\n",
            "101/101 [==============================] - 0s 277us/step - loss: 0.1896 - accuracy: 0.9208 - val_loss: 0.2207 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 277us/step - loss: 0.1896 - accuracy: 0.9208 - val_loss: 0.2207 - val_accuracy: 0.9615\n",
            "Epoch 74/100Epoch 74/100\n",
            "\n",
            "101/101 [==============================] - 0s 561us/step - loss: 0.1871 - accuracy: 0.9208 - val_loss: 0.2195 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 561us/step - loss: 0.1871 - accuracy: 0.9208 - val_loss: 0.2195 - val_accuracy: 0.9615\n",
            "Epoch 75/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1599 - accuracy: 1.0000Epoch 75/100\n",
            "101/101 [==============================] - 0s 252us/step - loss: 0.1837 - accuracy: 0.9208 - val_loss: 0.2162 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 252us/step - loss: 0.1837 - accuracy: 0.9208 - val_loss: 0.2162 - val_accuracy: 0.9615\n",
            "Epoch 76/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2094 - accuracy: 0.9000Epoch 76/100\n",
            "101/101 [==============================] - 0s 224us/step - loss: 0.1810 - accuracy: 0.9406 - val_loss: 0.2132 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 224us/step - loss: 0.1810 - accuracy: 0.9406 - val_loss: 0.2132 - val_accuracy: 0.9615\n",
            "Epoch 77/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1557 - accuracy: 1.0000Epoch 77/100\n",
            "101/101 [==============================] - 0s 273us/step - loss: 0.1786 - accuracy: 0.9406 - val_loss: 0.2104 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 273us/step - loss: 0.1786 - accuracy: 0.9406 - val_loss: 0.2104 - val_accuracy: 0.9615\n",
            "Epoch 78/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2423 - accuracy: 0.9000Epoch 78/100\n",
            "101/101 [==============================] - 0s 203us/step - loss: 0.1762 - accuracy: 0.9406 - val_loss: 0.2086 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 203us/step - loss: 0.1762 - accuracy: 0.9406 - val_loss: 0.2086 - val_accuracy: 0.9615\n",
            "Epoch 79/100\n",
            "Epoch 79/100\n",
            "101/101 [==============================] - 0s 234us/step - loss: 0.1740 - accuracy: 0.9505 - val_loss: 0.2069 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 234us/step - loss: 0.1740 - accuracy: 0.9505 - val_loss: 0.2069 - val_accuracy: 0.9615\n",
            "Epoch 80/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.0383 - accuracy: 1.0000Epoch 80/100\n",
            "101/101 [==============================] - 0s 223us/step - loss: 0.1718 - accuracy: 0.9505 - val_loss: 0.2048 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 223us/step - loss: 0.1718 - accuracy: 0.9505 - val_loss: 0.2048 - val_accuracy: 0.9615\n",
            "Epoch 81/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1195 - accuracy: 1.0000Epoch 81/100\n",
            "101/101 [==============================] - 0s 221us/step - loss: 0.1694 - accuracy: 0.9505 - val_loss: 0.2014 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 221us/step - loss: 0.1694 - accuracy: 0.9505 - val_loss: 0.2014 - val_accuracy: 0.9615\n",
            "Epoch 82/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.0793 - accuracy: 1.0000Epoch 82/100\n",
            "101/101 [==============================] - 0s 236us/step - loss: 0.1677 - accuracy: 0.9505 - val_loss: 0.1992 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 236us/step - loss: 0.1677 - accuracy: 0.9505 - val_loss: 0.1992 - val_accuracy: 0.9615\n",
            "Epoch 83/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2423 - accuracy: 0.8000Epoch 83/100\n",
            "101/101 [==============================] - 0s 283us/step - loss: 0.1654 - accuracy: 0.9505 - val_loss: 0.1977 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 283us/step - loss: 0.1654 - accuracy: 0.9505 - val_loss: 0.1977 - val_accuracy: 0.9615\n",
            "Epoch 84/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1321 - accuracy: 1.0000Epoch 84/100\n",
            "101/101 [==============================] - 0s 188us/step - loss: 0.1631 - accuracy: 0.9505 - val_loss: 0.1961 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 188us/step - loss: 0.1631 - accuracy: 0.9505 - val_loss: 0.1961 - val_accuracy: 0.9615\n",
            "Epoch 85/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1416 - accuracy: 1.0000Epoch 85/100\n",
            "101/101 [==============================] - 0s 210us/step - loss: 0.1610 - accuracy: 0.9505 - val_loss: 0.1948 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 210us/step - loss: 0.1610 - accuracy: 0.9505 - val_loss: 0.1948 - val_accuracy: 0.9615\n",
            "Epoch 86/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2637 - accuracy: 0.8000Epoch 86/100\n",
            "101/101 [==============================] - 0s 168us/step - loss: 0.1592 - accuracy: 0.9505 - val_loss: 0.1935 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 168us/step - loss: 0.1592 - accuracy: 0.9505 - val_loss: 0.1935 - val_accuracy: 0.9615\n",
            "Epoch 87/100\n",
            "Epoch 87/100\n",
            "101/101 [==============================] - 0s 275us/step - loss: 0.1571 - accuracy: 0.9505 - val_loss: 0.1912 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 275us/step - loss: 0.1571 - accuracy: 0.9505 - val_loss: 0.1912 - val_accuracy: 0.9615\n",
            "Epoch 88/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.2067 - accuracy: 0.9000Epoch 88/100\n",
            "101/101 [==============================] - 0s 225us/step - loss: 0.1550 - accuracy: 0.9505 - val_loss: 0.1894 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 225us/step - loss: 0.1550 - accuracy: 0.9505 - val_loss: 0.1894 - val_accuracy: 0.9615\n",
            "Epoch 89/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1178 - accuracy: 1.0000Epoch 89/100\n",
            "101/101 [==============================] - 0s 218us/step - loss: 0.1531 - accuracy: 0.9505 - val_loss: 0.1880 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 218us/step - loss: 0.1531 - accuracy: 0.9505 - val_loss: 0.1880 - val_accuracy: 0.9615\n",
            "Epoch 90/100\n",
            "Epoch 90/100\n",
            "101/101 [==============================] - 0s 241us/step - loss: 0.1508 - accuracy: 0.9505 - val_loss: 0.1846 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 241us/step - loss: 0.1508 - accuracy: 0.9505 - val_loss: 0.1846 - val_accuracy: 0.9615\n",
            "Epoch 91/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1271 - accuracy: 1.0000Epoch 91/100\n",
            "101/101 [==============================] - 0s 210us/step - loss: 0.1489 - accuracy: 0.9505 - val_loss: 0.1831 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 210us/step - loss: 0.1489 - accuracy: 0.9505 - val_loss: 0.1831 - val_accuracy: 0.9615\n",
            "Epoch 92/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.0741 - accuracy: 1.0000Epoch 92/100\n",
            "101/101 [==============================] - 0s 222us/step - loss: 0.1464 - accuracy: 0.9505 - val_loss: 0.1845 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 222us/step - loss: 0.1464 - accuracy: 0.9505 - val_loss: 0.1845 - val_accuracy: 0.9615\n",
            "Epoch 93/100\n",
            "Epoch 93/100\n",
            "101/101 [==============================] - 0s 279us/step - loss: 0.1451 - accuracy: 0.9505 - val_loss: 0.1841 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 279us/step - loss: 0.1451 - accuracy: 0.9505 - val_loss: 0.1841 - val_accuracy: 0.9615\n",
            "Epoch 94/100\n",
            "Epoch 94/100\n",
            "101/101 [==============================] - 0s 266us/step - loss: 0.1433 - accuracy: 0.9505 - val_loss: 0.1826 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 266us/step - loss: 0.1433 - accuracy: 0.9505 - val_loss: 0.1826 - val_accuracy: 0.9615\n",
            "Epoch 95/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.0532 - accuracy: 1.0000Epoch 95/100\n",
            "101/101 [==============================] - 0s 294us/step - loss: 0.1416 - accuracy: 0.9505 - val_loss: 0.1811 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 294us/step - loss: 0.1416 - accuracy: 0.9505 - val_loss: 0.1811 - val_accuracy: 0.9615\n",
            "Epoch 96/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1569 - accuracy: 0.9000Epoch 96/100\n",
            "101/101 [==============================] - 0s 267us/step - loss: 0.1402 - accuracy: 0.9505 - val_loss: 0.1791 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 267us/step - loss: 0.1402 - accuracy: 0.9505 - val_loss: 0.1791 - val_accuracy: 0.9615\n",
            "Epoch 97/100\n",
            "Epoch 97/100\n",
            "101/101 [==============================] - 0s 318us/step - loss: 0.1383 - accuracy: 0.9505 - val_loss: 0.1777 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 318us/step - loss: 0.1383 - accuracy: 0.9505 - val_loss: 0.1777 - val_accuracy: 0.9615\n",
            "Epoch 98/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.0678 - accuracy: 1.0000Epoch 98/100\n",
            "101/101 [==============================] - 0s 236us/step - loss: 0.1363 - accuracy: 0.9505 - val_loss: 0.1753 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 236us/step - loss: 0.1363 - accuracy: 0.9505 - val_loss: 0.1753 - val_accuracy: 0.9615\n",
            "Epoch 99/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1473 - accuracy: 0.9000Epoch 99/100\n",
            "101/101 [==============================] - 0s 250us/step - loss: 0.1341 - accuracy: 0.9604 - val_loss: 0.1729 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 250us/step - loss: 0.1341 - accuracy: 0.9604 - val_loss: 0.1729 - val_accuracy: 0.9615\n",
            "Epoch 100/100\n",
            " 10/101 [=>............................] - ETA: 0s - loss: 0.1440 - accuracy: 1.0000Epoch 100/100\n",
            "101/101 [==============================] - 0s 279us/step - loss: 0.1328 - accuracy: 0.9604 - val_loss: 0.1705 - val_accuracy: 0.9615\n",
            "101/101 [==============================] - 0s 279us/step - loss: 0.1328 - accuracy: 0.9604 - val_loss: 0.1705 - val_accuracy: 0.9615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f5bee22dfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f5bee22dfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNOZ0q5M5jEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred=model1.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqH5Q8Yz7zUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_class=np.argmax(y_pred,axis=1)\n",
        "# y_test_class=np.argmax(y_test,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "revTpHsi8U4r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "0c8c9416-e986-4c04-a5f8-030fa3882397"
      },
      "source": [
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVLbuQOZ7_xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cn=confusion_matrix(y_test,y_pred_class)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2RrGCr9BO7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsEY_kEnBVS7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "6cc29fec-1fbd-4bf6-90ae-6faef91775b7"
      },
      "source": [
        "print(classification_report(y_test,y_pred_class))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.86      0.92         7\n",
            "           1       0.75      0.60      0.67        10\n",
            "           2       0.56      0.83      0.67         6\n",
            "\n",
            "    accuracy                           0.74        23\n",
            "   macro avg       0.77      0.76      0.75        23\n",
            "weighted avg       0.78      0.74      0.74        23\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJfc7qsN8XAY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "9cf320e4-44e3-4b4f-950e-f391bd67b8d5"
      },
      "source": [
        "sns.heatmap(cn,annot=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5bec75f278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zamluSq-Aqoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}